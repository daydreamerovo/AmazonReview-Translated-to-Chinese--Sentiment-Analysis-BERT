{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4bb0691",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "找到了 5006 条中性评论用于数据增强。\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>final_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>我第一次是在名古屋的一家商店里听到这首歌的，立刻爱上了凯莎（Ke$ha）的《Tik Tok》...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1</td>\n",
       "      <td>“这是一次芝加哥乐队的回顾专辑，这次是进口版。问题和以往一样。哥伦比亚唱片公司的录音远远胜过...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1</td>\n",
       "      <td>一般来说，我通常很享受翻唱专辑。但这张专辑例外。它既没有让汤姆·佩蒂也没有让卢·威廉姆斯得到...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>1</td>\n",
       "      <td>这张录音是1979年11月在布鲁塞尔为比利时的Philippe Defalle唱片公司录制的...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>1</td>\n",
       "      <td>“我对这张CD非常失望，因为里面没有歌词。&lt;br /&gt;这些只是录音室的伴奏带。&lt;br /&gt;虽...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     sentiment                                         final_text\n",
       "4            1  我第一次是在名古屋的一家商店里听到这首歌的，立刻爱上了凯莎（Ke$ha）的《Tik Tok》...\n",
       "22           1  “这是一次芝加哥乐队的回顾专辑，这次是进口版。问题和以往一样。哥伦比亚唱片公司的录音远远胜过...\n",
       "23           1  一般来说，我通常很享受翻唱专辑。但这张专辑例外。它既没有让汤姆·佩蒂也没有让卢·威廉姆斯得到...\n",
       "69           1  这张录音是1979年11月在布鲁塞尔为比利时的Philippe Defalle唱片公司录制的...\n",
       "127          1  “我对这张CD非常失望，因为里面没有歌词。<br />这些只是录音室的伴奏带。<br />虽..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "import sys\n",
    "\n",
    "# Register tqdm for pandas progress_apply\n",
    "tqdm.pandas()\n",
    "# Load your final 3-class dataset\n",
    "final_output_file = '../data/final_training_data_3class.csv'\n",
    "\n",
    "df = pd.read_csv(final_output_file)\n",
    "\n",
    "# Filter out only the neutral reviews (where sentiment == 1)\n",
    "neutral_df = df[df['sentiment'] == 1].copy()\n",
    "\n",
    "print(f\"找到了 {len(neutral_df)} 条中性评论用于数据增强。\")\n",
    "neutral_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3a9b7c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dashscope\n",
    "from src import config # We need this to get the API key and model name\n",
    "import time\n",
    "\n",
    "# --- NEW Back-translation function using Qwen API ---\n",
    "\n",
    "# First, make sure the API key is set up\n",
    "# This function is from our previous train.py, we can reuse it here\n",
    "def setup_api_key():\n",
    "    if not config.QWEN_API_KEY:\n",
    "        raise ValueError(\"QWEN_API_KEY environment variable not set.\")\n",
    "    dashscope.api_key = config.QWEN_API_KEY\n",
    "\n",
    "# Call it once to set the key\n",
    "setup_api_key()\n",
    "\n",
    "def back_translate_qwen(text, lang='fr'):\n",
    "    \"\"\"\n",
    "    Back-translates using the stable Qwen API.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # --- Step 1: Chinese to Foreign Language ---\n",
    "        prompt1 = f\"Please translate the following Chinese text to {lang}. Only return the translated text. Chinese text: '{text}'\"\n",
    "        response1 = dashscope.Generation.call(\n",
    "            model=config.QWEN_MODEL_NAME, # We can use the fast 'qwen-turbo'\n",
    "            prompt=prompt1\n",
    "        )\n",
    "        if response1.status_code != 200:\n",
    "            print(f\"Error in first translation step: {response1.message}\")\n",
    "            return None\n",
    "        translated_text = response1.output.strip()\n",
    "\n",
    "        # Add a very small delay, as official APIs are more robust\n",
    "        time.sleep(0.2)\n",
    "\n",
    "        # --- Step 2: Foreign Language back to Chinese ---\n",
    "        prompt2 = f\"Please translate the following {lang} text to Chinese. Only return the translated text. {lang.capitalize()} text: '{translated_text}'\"\n",
    "        response2 = dashscope.Generation.call(\n",
    "            model=config.QWEN_MODEL_NAME,\n",
    "            prompt=prompt2\n",
    "        )\n",
    "        if response2.status_code != 200:\n",
    "            print(f\"Error in second translation step: {response2.message}\")\n",
    "            return None\n",
    "            \n",
    "        back_translated_text = response2.output.strip()\n",
    "        \n",
    "        return back_translated_text\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred during Qwen back-translation: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3435f5ed",
   "metadata": {},
   "source": [
    "#### 将经过dataaugumentation代码数据增强之后的含英文数据进行最后的清洗："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d90503f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在加载增强后的数据，来源: ../data/final_training_data_augmented.csv\n",
      "加载完成，共有 111644 行数据。\n",
      "正在筛选，只保留包含中文的评论...\n",
      "筛选完成！包含中文的行数: 29633\n",
      "被剔除的英文行数: 82011\n",
      "\n",
      "--- 最终清洗完成！ ---\n",
      "一个全新的、只包含中文的数据集已保存到: ../data/TRAINING_DATASET_FINAL.csv\n",
      "这个文件才是我们应该用于最终训练的文件。\n",
      "\n",
      "最终纯净数据预览:\n",
      "   sentiment                                         final_text\n",
      "0          1  柴可夫斯基并没有轻率地创作他的第四交响曲；他表达了一个宏大的主题，总结了他许多内心的斗争。这...\n",
      "4          1  这张CD并没有我预期的那么好。这张专辑最好的歌曲是《Home Sweet Home》的翻唱版...\n",
      "6          2  这是一支引人入胜的复古金属乐队。我之前对他们的作品并不十分印象深刻，但在我看来，这张专辑是他...\n",
      "7          1  这张专辑上有一些有趣的混音，其中一半对我来说非常出色，一个很有趣，还有一个我认为不好。让听众...\n",
      "9          2                                    “我只希望它能有更多歌曲……”\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# --- 这是确保数据集100%纯净的最后一步 ---\n",
    "\n",
    "# 1. 加载你最新的、经过数据增强的数据集\n",
    "source_file = '../data/final_training_data_augmented.csv'\n",
    "# 这将是我们用于训练的、最最最终版的文件\n",
    "final_clean_file = '../data/TRAINING_DATASET_FINAL_DATAAUG.csv'\n",
    "\n",
    "print(f\"正在加载增强后的数据，来源: {source_file}\")\n",
    "df = pd.read_csv(source_file)\n",
    "print(f\"加载完成，共有 {len(df)} 行数据。\")\n",
    "\n",
    "# 2. 定义一个函数，用来检查文本中是否包含至少一个中文字符\n",
    "def contains_chinese(text):\n",
    "    \"\"\"\n",
    "    Returns True if the text contains at least one Chinese character.\n",
    "    \"\"\"\n",
    "    # [\\u4e00-\\u9fa5] 是所有中文字符的Unicode范围\n",
    "    return bool(re.search(r'[\\u4e00-\\u9fa5]', str(text)))\n",
    "\n",
    "# 3. 筛选出所有包含中文的行\n",
    "print(\"正在筛选，只保留包含中文的评论...\")\n",
    "# .apply() 方法会将函数应用到'final_text'列的每一行\n",
    "is_chinese_mask = df['final_text'].apply(contains_chinese)\n",
    "\n",
    "# 使用这个布尔值的\"mask\"来过滤DataFrame，只保留为True的行\n",
    "df_clean = df[is_chinese_mask].copy()\n",
    "\n",
    "print(f\"筛选完成！包含中文的行数: {len(df_clean)}\")\n",
    "print(f\"被剔除的英文行数: {len(df) - len(df_clean)}\")\n",
    "\n",
    "# 4. 保存最终的、纯净的中文数据集\n",
    "df_clean.to_csv(final_clean_file, index=False, encoding='utf-8-sig')\n",
    "\n",
    "print(f\"\\n--- 最终清洗完成！ ---\")\n",
    "print(f\"一个全新的、只包含中文的数据集已保存到: {final_clean_file}\")\n",
    "print(\"这个文件才是我们应该用于最终训练的文件。\")\n",
    "print(\"\\n最终纯净数据预览:\")\n",
    "print(df_clean.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "851b61ed",
   "metadata": {},
   "source": [
    "### 清洗3分类数据\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2630b077",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting cleaning process for ../data/final_training_data_3class_cleaned.csv ---\n",
      "Successfully loaded 17958 rows.\n",
      "Normalizing text (removing HTML, extra spaces, etc.)...\n",
      "Filtering out rows that are still in English...\n",
      "0 English-only rows were removed.\n",
      "\n",
      "--- Cleaning complete! ---\n",
      "Final dataset has 17958 rows.\n",
      "Clean file saved to: ../data/final_training_data_3class_cleaned.csv\n",
      "\n",
      "Preview of the final clean data:\n",
      "   sentiment                                         final_text\n",
      "0          2  如果我每听一次这张CD就有一美元每回让艾丽卡Alexa播放一次就有一美元的话我现在就会很有钱...\n",
      "1          2                    出色的音效迫不及待想亲眼看到他们了当他们在城里时我总是想念他们\n",
      "2          2                  这是一张很棒的CD音乐很好播放也很顺畅卖家回复非常快三天内就收到了\n",
      "3          0                  这些不是真正的德国歌手他们有口音这和他们宣传的完全不一样音乐太差了\n",
      "4          1  我第一次是在名古屋的一家商店里听到这首歌的立刻爱上了凯莎Keha的Tik Tok混音版在日本...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# --- Configuration ---\n",
    "# Input file: The 3-class dataset without augmentation \n",
    "source_file = '../data/final_training_data_3class.csv'\n",
    "# Output file: A new, clean version of the above\n",
    "output_file = '../data/final_training_data_3class_cleaned.csv'\n",
    "\n",
    "print(f\"--- Starting cleaning process for {source_file} ---\")\n",
    "\n",
    "# --- 1. Load the dataset ---\n",
    "try:\n",
    "    df = pd.read_csv(source_file)\n",
    "    print(f\"Successfully loaded {len(df)} rows.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: File not found at {source_file}. Please make sure the file exists.\")\n",
    "    # In a notebook, this will stop the execution of this cell\n",
    "    raise\n",
    "\n",
    "# --- 2. Define a comprehensive text cleaning function ---\n",
    "def clean_and_normalize(text):\n",
    "    \"\"\"\n",
    "    This function cleans and normalizes a single piece of text.\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    # Remove HTML tags (e.g., <br>)\n",
    "    text = re.sub(r'<.*?>', ' ', text)\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+', ' ', text)\n",
    "    # Replace newline characters and tabs with a space\n",
    "    text = text.replace('\\n', ' ').replace('\\r', ' ').replace('\\t', ' ')\n",
    "    # Optional: Remove non-essential special characters, \n",
    "    # keeping Chinese, English, numbers, and basic punctuation.\n",
    "    # You can customize the characters you want to keep inside the brackets.\n",
    "    text = re.sub(r'[^\\u4e00-\\u9fa5a-zA-Z0-9\\s.,!?\\'\"]', '', text)\n",
    "    # Replace multiple spaces with a single space\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "print(\"Normalizing text (removing HTML, extra spaces, etc.)...\")\n",
    "# Apply the function to create a new clean column, let's call it 'cleaned_text' for now\n",
    "df['cleaned_text'] = df['final_text'].apply(clean_and_normalize)\n",
    "\n",
    "# --- 3. Define a function to check for Chinese characters ---\n",
    "def contains_chinese(text):\n",
    "    \"\"\"\n",
    "    Returns True if the text contains at least one Chinese character.\n",
    "    \"\"\"\n",
    "    return bool(re.search(r'[\\u4e00-\\u9fa5]', str(text)))\n",
    "\n",
    "# --- 4. Filter out rows that do not contain Chinese text ---\n",
    "print(\"Filtering out rows that are still in English...\")\n",
    "original_rows = len(df)\n",
    "# Create a boolean mask: True for rows with Chinese, False otherwise\n",
    "is_chinese_mask = df['cleaned_text'].apply(contains_chinese)\n",
    "# Keep only the rows where the mask is True\n",
    "df_clean = df[is_chinese_mask].copy()\n",
    "removed_rows = original_rows - len(df_clean)\n",
    "print(f\"{removed_rows} English-only rows were removed.\")\n",
    "\n",
    "# --- 5. Select final columns and save the file ---\n",
    "# We only need the sentiment and the cleaned text\n",
    "final_df = df_clean[['sentiment', 'cleaned_text']]\n",
    "# Rename 'cleaned_text' back to 'final_text' for consistency with the training script\n",
    "final_df = final_df.rename(columns={'cleaned_text': 'final_text'})\n",
    "\n",
    "final_df.to_csv(output_file, index=False, encoding='utf-8-sig')\n",
    "\n",
    "print(f\"\\n--- Cleaning complete! ---\")\n",
    "print(f\"Final dataset has {len(final_df)} rows.\")\n",
    "print(f\"Clean file saved to: {output_file}\")\n",
    "print(\"\\nPreview of the final clean data:\")\n",
    "print(final_df.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_new",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
