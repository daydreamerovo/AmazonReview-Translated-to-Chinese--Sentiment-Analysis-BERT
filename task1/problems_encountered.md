第一周任务过程复盘与问题记录

基于电商评论的情感分类Pipeline搭建

2025年8月6日



&nbsp;最终产出：

1\.  中文翻译后的电商领域数据集:

&nbsp;    源数据: McAuley-Lab/Amazon-Reviews-2023, `Digital\_Music` 类目。

&nbsp;    处理后: 翻译了10万条评论，并经过数据清洗，最终生成了可用于模型训练的 `final\_training\_data.csv` 文件。

2\.  基于BERT的情感分类Pipeline文档:

&nbsp;    本次任务的所有代码共同构成了一个完整的情感分类Pipeline。





过程中遇到的主要问题与解决方案记录

数据获取与准备



&nbsp;问题: 使用Hugging Face `datasets`库加载数据时，因库版本更新，遭遇`RuntimeError: Dataset scripts are no longer supported`错误，无法下载。



&nbsp;分析: `datasets`库的新版本不再支持执行远程的加载脚本。

&nbsp; 解决: 最终采用了降级`datasets`库版本到 `2.16.0` 的方法，使其能够兼容旧的数据集加载方式，成功获取了数据。



***数据翻译***

&nbsp;问题：在翻译10万条评论的过程中，遇到了API额度耗尽、脚本因各种错误中断、进度丢失、效率低下等一系列工程挑战。



分析：API额度耗尽: 在翻译到约38%时，阿里云百炼API的免费额度用尽，程序开始持续返回 `Arrearage` (欠费) 错误。

解决: 明确了这是一个资源问题而非代码问题，对后续费用进行了估算，总花费为（150 - 136.56） 元，充值150元，翻译完成后剩136.56元。



 ***脚本健壮性不足:***



&nbsp;问题：初始脚本在被`Ctrl+C`中断后，所有在内存中已翻译的数据全部丢失。

&nbsp;          高并发请求容易触发服务器限速，或因用Excel打开CSV文件导致文件被锁定，使程序看起来“卡住”。

&nbsp;          遇到了`SSLError` (网络中断), `\_typ` (疑似库的线程安全问题), `AttributeError`  等多种错误。



解决:将脚本重构为支持“断点续传”和“定期保存”的健壮模式。这确保了即使程序中断，也只会丢失少量进度。

&nbsp;       在`config.py`中增加了`CONCURRENT\_THREADS`参数，通过降低并发数（15），成功缓解了服务器限速问题。

&nbsp;       逐一修复了由数据结构处理不当、文件锁定、库版本兼容性等引起的多个BUG，最终得到一个稳定可靠的并发翻译脚本。



***环境配置与模型训练***

&nbsp;问题: Jupyter Notebook导入失败 (`ModuleNotFoundError`): 在Notebook中无法导入`src`目录下的自定义模块。



分析: Notebook的工作目录与其`.ipynb`文件位置绑定，导致它找不到位于上级目录的`src`文件夹。

解决: 在Notebook开头通过`sys.path.append('../')`，临时将项目根目录加入到Python的搜索路径中，解决了模块导入问题。



PyTorch与GPU驱动不兼容: 安装的PyTorch稳定版不支持最新的GPU（RTX 5060），导致无法使用GPU加速，并报错`torch.AcceleratorError`。

解决: 下载128版本







***模型评估与调优：***

问题: 初版模型的评估报告显示总体准确率(Accuracy)很高(81%)，但对少数类别（如2星、4星）的识别能力极差，`macro avg f1-score`很低。



分析: 问题的根源是数据类别严重不平衡。模型通过“猜”样本量最大的5星类别，获得了虚高的总体准确率，但这并不是一个真正有用的模型。

解决: 评估层面: 在`train\_test\_split`中加入`stratify`参数进行分层抽样，确保验证集的数据分布与整体一致，使评估结果更真实、更可信。

&nbsp;        训练层面: 在损失函数`CrossEntropyLoss`中加入`class\_weights`参数，对少数类别施加更高的“惩罚”，迫使模型去关注和学习这些难以识别的样本。



